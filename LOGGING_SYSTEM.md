# üìä Syst√®me de Logging - FootIntel API

## Vue d'ensemble

Syst√®me de logging structur√© et organis√© pour une observabilit√© compl√®te de l'application.

---

## üèóÔ∏è Architecture

### Niveaux de logging

```python
# Hi√©rarchie des loggers
footintel                      # Logger racine
‚îú‚îÄ‚îÄ api                        # Endpoints API
‚îÇ   ‚îú‚îÄ‚îÄ api.auth              # Authentification
‚îÇ   ‚îú‚îÄ‚îÄ api.analyze           # Analyses de matchs
‚îÇ   ‚îú‚îÄ‚îÄ api.coupons           # Gestion des coupons
‚îÇ   ‚îî‚îÄ‚îÄ api.subscription      # Abonnements
‚îú‚îÄ‚îÄ services                   # Services m√©tier
‚îÇ   ‚îú‚îÄ‚îÄ services.ai           # Service IA (Gemini)
‚îÇ   ‚îú‚îÄ‚îÄ services.cache        # Service de cache Redis
‚îÇ   ‚îî‚îÄ‚îÄ services.scrapers     # Scrapers web
‚îú‚îÄ‚îÄ providers                  # Fournisseurs de donn√©es
‚îÇ   ‚îî‚îÄ‚îÄ providers.football_data_org
‚îî‚îÄ‚îÄ celery                     # T√¢ches asynchrones
```

### Types de logs

| Type | Niveau | Usage | Emoji |
|------|--------|-------|-------|
| **HTTP Requests** | INFO | Entr√©e/sortie des requ√™tes | ‚Üí ‚úì ‚úó |
| **Auth Events** | INFO | Login, logout, registration | üîê |
| **AI Analysis** | INFO | Analyses Gemini | ü§ñ |
| **External API** | INFO | Appels API externes | üì° |
| **Cache Operations** | DEBUG | Redis GET/SET/HIT/MISS | üíæ |
| **Database** | DEBUG | Requ√™tes SQL | üóÑÔ∏è |
| **Errors** | ERROR | Erreurs r√©cup√©rables | ‚ùå |
| **Critical** | CRITICAL | Erreurs syst√®me | üö® |

---

## üìÅ Structure des fichiers de logs

```
backend/
‚îî‚îÄ‚îÄ logs/
    ‚îú‚îÄ‚îÄ app.log          # Tous les logs (INFO+)
    ‚îî‚îÄ‚îÄ error.log        # Erreurs uniquement (ERROR+)
```

### Configuration de rotation

- **Taille maximale** : 10 MB par fichier
- **Backups** : 5 fichiers pour app.log, 10 pour error.log
- **Encodage** : UTF-8

---

## üé® Formats de sortie

### Console (d√©veloppement)

```
16:42:31 - INFO     [req:a3f2d8b1] [user:e5c9a2b4] - footintel.api.auth.login:184 - ‚úÖ Login successful: user@example.com
```

**Couleurs** :
- üîµ DEBUG : Cyan
- üü¢ INFO : Vert
- üü° WARNING : Jaune
- üî¥ ERROR : Rouge
- üü£ CRITICAL : Magenta

### Fichier (production)

```json
{
  "timestamp": "2024-01-15T16:42:31.123456",
  "level": "INFO",
  "logger": "footintel.api.auth",
  "message": "‚úÖ Login successful: user@example.com",
  "module": "auth",
  "function": "login",
  "line": 184,
  "request_id": "a3f2d8b1-4e5f-6g7h-8i9j-0k1l2m3n4o5p",
  "user_id": "e5c9a2b4-1234-5678-9abc-def123456789",
  "extra_data": {
    "email": "user@example.com",
    "subscription_plan": "premium"
  }
}
```

---

## üîß Utilisation

### 1. Importer le logger

```python
from app.core.logger import get_logger

logger = get_logger('api.endpoint_name')  # Nom hi√©rarchique
```

### 2. Logs standards

```python
# Information
logger.info("Processing data")

# Debug (uniquement si DEBUG=true)
logger.debug("Variable value: {value}")

# Warning
logger.warning("Rate limit approaching")

# Error avec traceback
logger.error("Failed to process", exc_info=True)
```

### 3. Logs structur√©s avec contexte

```python
logger.info(
    "User action completed",
    extra={'extra_data': {
        'user_id': user.id,
        'action': 'subscription_upgrade',
        'plan': 'premium',
        'duration_ms': 123.45
    }}
)
```

### 4. M√©thodes sp√©cialis√©es (LoggerAdapter)

#### Requ√™tes HTTP
```python
logger.log_request(
    method="POST",
    path="/api/v1/analyze",
    status_code=200,
    duration_ms=456.78
)
```

#### √âv√©nements d'authentification
```python
logger.log_auth(
    event='login_success',
    user_id=str(user.id),
    email=user.email,
    success=True
)
```

#### Appels API externes
```python
logger.log_external_api(
    service='Football-Data.org',
    endpoint='fixtures',
    status='200',
    duration_ms=234.56
)
```

#### Op√©rations de cache
```python
logger.log_cache(
    operation='GET',
    key='fixtures:2024-01-15',
    hit=True  # ou False pour MISS
)
```

#### Analyses IA
```python
logger.log_ai_analysis(
    fixture_id=12345,
    user_id=str(user.id),
    duration_ms=2345.67,
    success=True
)
```

#### Paiements
```python
logger.log_payment(
    event='subscription_created',
    user_id=str(user.id),
    plan='premium',
    amount=9.99,
    success=True
)
```

---

## üîç Contexte de requ√™te (request_id, user_id)

### Middleware automatique

Le middleware dans `main.py` injecte automatiquement :
- `request_id` : UUID unique par requ√™te
- `user_id` : ID de l'utilisateur authentifi√© (si disponible)

Ces valeurs sont ajout√©es automatiquement √† **tous les logs** pendant le traitement de la requ√™te.

### Acc√®s manuel

```python
from app.core.logger import set_request_context, clear_request_context

# D√©finir le contexte
set_request_context(request_id="abc-123", user_id="user-456")

# Tous les logs auront ces valeurs
logger.info("Action")  # Contient request_id et user_id automatiquement

# Nettoyer le contexte
clear_request_context()
```

---

## üìç Points de logging cl√©s

### 1. API Endpoints (`app/api/v1/`)

```python
# D√©but de traitement
logger.info(
    f"üîÑ Processing {operation}",
    extra={'extra_data': {'param': value}}
)

# Succ√®s
logger.info(
    f"‚úÖ {operation} completed",
    extra={'extra_data': {'result': data}}
)

# Erreur
logger.error(
    f"‚ùå {operation} failed: {error}",
    exc_info=True,
    extra={'extra_data': {'error': str(error)}}
)
```

### 2. Services AI (`app/services/ai_service.py`)

```python
# D√©but d'analyse
logger.info(
    f"ü§ñ Starting AI analysis: {home_team} vs {away_team}",
    extra={'extra_data': {
        'home_team': home_team,
        'away_team': away_team
    }}
)

# Succ√®s avec timing
logger.log_ai_analysis(
    fixture_id=fixture_id,
    user_id=user_id,
    duration_ms=duration_ms,
    success=True
)
```

### 3. Cache (`app/services/cache_service.py`)

```python
# Cache HIT
logger.log_cache('GET', key, hit=True)

# Cache MISS
logger.log_cache('GET', key, hit=False)

# Cache SET
logger.log_cache('SET', key)
logger.debug(
    f"üíæ Cached: {key} (TTL: {expire}s)",
    extra={'extra_data': {'key': key, 'ttl': expire}}
)
```

### 4. Providers (`app/providers/football/`)

```python
# Appel API
logger.debug(
    f"üì° Football-Data.org API call: {endpoint}",
    extra={'extra_data': {'endpoint': endpoint, 'params': params}}
)

# Succ√®s
logger.log_external_api(
    'Football-Data.org',
    endpoint,
    f"{response.status_code}",
    duration_ms
)

# Erreur 403 (free tier)
logger.warning(
    f"‚ö†Ô∏è Football-Data.org 403 Forbidden: {endpoint}",
    extra={'extra_data': {'endpoint': endpoint, 'status_code': 403}}
)
```

### 5. Authentification (`app/api/v1/auth.py`)

```python
# Tentative de login
logger.log_auth(
    'login_attempt',
    email=email,
    success=False
)

# Login r√©ussi
logger.log_auth(
    'login_success',
    user_id=str(user.id),
    email=user.email,
    success=True
)

# Inscription
logger.log_auth(
    'user_registered',
    user_id=str(user.id),
    email=user.email,
    success=True
)
```

---

## üõ†Ô∏è Configuration

### Variables d'environnement

```bash
# .env
DEBUG=false  # true pour DEBUG level, false pour INFO level
```

### Niveaux par module

```python
# backend/app/core/logger.py
logging.getLogger("uvicorn.access").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("sqlalchemy.engine").setLevel(logging.WARNING)
```

---

## üìä Monitoring et analyse

### Recherche dans les logs JSON

```bash
# Filtrer par niveau
cat logs/app.log | jq 'select(.level == "ERROR")'

# Filtrer par request_id
cat logs/app.log | jq 'select(.request_id == "a3f2d8b1-...")'

# Filtrer par type d'√©v√©nement
cat logs/app.log | jq 'select(.extra_data.type == "external_api")'

# Analyses AI lentes (> 2s)
cat logs/app.log | jq 'select(.extra_data.type == "ai_analysis" and .extra_data.duration_ms > 2000)'
```

### Agr√©gation des m√©triques

```bash
# Compter les erreurs par logger
cat logs/error.log | jq -r '.logger' | sort | uniq -c

# Temps moyen de r√©ponse API
cat logs/app.log | jq 'select(.extra_data.type == "http_request") | .extra_data.duration_ms' | jq -s 'add/length'

# Top 10 endpoints les plus lents
cat logs/app.log | jq 'select(.extra_data.type == "http_request") | "\(.extra_data.path) \(.extra_data.duration_ms)"' -r | sort -k2 -n | tail -10
```

---

## üîí S√©curit√©

### Donn√©es sensibles

**NE JAMAIS logger** :
- ‚ùå Mots de passe
- ‚ùå Tokens JWT complets
- ‚ùå Cl√©s API compl√®tes
- ‚ùå Num√©ros de carte bancaire
- ‚ùå OTP codes

**Acceptables** :
- ‚úÖ Emails (pour tra√ßabilit√©)
- ‚úÖ User IDs (UUID)
- ‚úÖ Request IDs
- ‚úÖ Premi√®res/derni√®res lettres de tokens (pour debug)

### Exemple s√©curis√©

```python
# ‚ùå MAUVAIS
logger.info(f"User token: {full_token}")

# ‚úÖ BON
logger.info(
    f"Token generated for user",
    extra={'extra_data': {
        'user_id': user.id,
        'token_prefix': full_token[:8] + '...'
    }}
)
```

---

## üöÄ Bonnes pratiques

1. **Utiliser des emojis** pour une lecture rapide
   ```python
   logger.info("üöÄ Starting application")
   logger.error("‚ùå Database connection failed")
   ```

2. **Toujours inclure le contexte**
   ```python
   logger.error(
       f"Failed to process order",
       exc_info=True,  # Traceback complet
       extra={'extra_data': {
           'order_id': order_id,
           'user_id': user_id
       }}
   )
   ```

3. **Mesurer les dur√©es**
   ```python
   start_time = time.time()
   # ... op√©ration ...
   duration_ms = (time.time() - start_time) * 1000
   logger.info(f"Operation completed ({duration_ms:.2f}ms)")
   ```

4. **Logs progressifs pour op√©rations longues**
   ```python
   logger.info("Step 1/3: Fetching data")
   # ...
   logger.info("Step 2/3: Processing")
   # ...
   logger.info("Step 3/3: Saving results")
   ```

5. **Diff√©rencier dev/prod**
   ```python
   if settings.debug:
       logger.debug(f"Raw API response: {response_data}")
   logger.info("API call successful")
   ```

---

## üìà M√©triques de performance

### Logs √† surveiller

| M√©trique | Seuil | Action |
|----------|-------|--------|
| **HTTP response time** | > 2000ms | Optimiser endpoint |
| **AI analysis time** | > 3000ms | V√©rifier Gemini API |
| **External API time** | > 1000ms | Ajouter cache |
| **Cache hit rate** | < 70% | Augmenter TTL |
| **Error rate** | > 1% | Investiguer |

### Dashboard recommand√©

Utiliser un outil comme **Grafana** ou **Kibana** pour visualiser :
- Taux de succ√®s/√©chec par endpoint
- Temps de r√©ponse moyens
- Taux de cache HIT/MISS
- Distribution des erreurs par logger

---

## üîÑ Int√©gration CI/CD

### V√©rification pr√©-d√©ploiement

```bash
# V√©rifier qu'il n'y a pas de secrets dans les logs
grep -r "password\|secret\|api_key" logs/

# Analyser les erreurs critiques
cat logs/error.log | jq 'select(.level == "CRITICAL")'
```

### Alertes production

Configurer des alertes pour :
- `CRITICAL` logs ‚Üí Alerte imm√©diate
- `ERROR` > 10/min ‚Üí Alerte urgente
- Temps de r√©ponse > 5s ‚Üí Avertissement

---

## üìö Exemples complets

### Endpoint d'analyse de match

```python
from app.core.logger import get_logger
import time

logger = get_logger('api.analyze')

@router.post("/analyze")
async def analyze_match(fixture_id: int, user: User):
    start_time = time.time()
    
    logger.info(
        f"üîÑ Starting match analysis",
        extra={'extra_data': {
            'fixture_id': fixture_id,
            'user_id': str(user.id)
        }}
    )
    
    try:
        # Fetch data
        logger.debug("Fetching fixture data")
        fixture = await get_fixture(fixture_id)
        
        # AI Analysis
        logger.info(f"ü§ñ Requesting AI analysis")
        analysis = await ai_service.analyze(fixture)
        
        duration_ms = (time.time() - start_time) * 1000
        
        logger.log_ai_analysis(
            fixture_id=fixture_id,
            user_id=str(user.id),
            duration_ms=duration_ms,
            success=True
        )
        
        logger.info(
            f"‚úÖ Analysis completed ({duration_ms:.0f}ms)",
            extra={'extra_data': {
                'fixture_id': fixture_id,
                'confidence': analysis.confidence
            }}
        )
        
        return analysis
        
    except Exception as e:
        duration_ms = (time.time() - start_time) * 1000
        
        logger.error(
            f"‚ùå Analysis failed ({duration_ms:.0f}ms): {str(e)}",
            exc_info=True,
            extra={'extra_data': {
                'fixture_id': fixture_id,
                'user_id': str(user.id),
                'error': str(e)
            }}
        )
        
        raise HTTPException(500, "Analysis failed")
```

---

## ‚úÖ Checklist de migration

- [x] Logger racine configur√© avec rotation
- [x] Middleware HTTP pour request_id
- [x] Logs dans `ai_service.py`
- [x] Logs dans `cache_service.py`
- [x] Logs dans `football_data_org.py`
- [x] Logs dans `scrapers.py`
- [x] Logs dans `auth.py` (register, login)
- [ ] Logs dans `analyze.py`
- [ ] Logs dans `coupons.py`
- [ ] Logs dans `subscription.py`
- [ ] Logs dans `stripe_service.py`
- [ ] Logs dans t√¢ches Celery

---

## üéØ Prochaines √©tapes

1. Compl√©ter les logs dans endpoints restants
2. Ajouter logs dans services de paiement (Stripe, Moneroo)
3. Logger les t√¢ches Celery (emails, renewals)
4. Configurer monitoring externe (Sentry, Datadog)
5. Cr√©er dashboard Grafana pour m√©triques temps r√©el

---

**Documentation mise √† jour** : 2024-01-15  
**Version** : 1.0
