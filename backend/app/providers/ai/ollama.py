import json
import httpx
import asyncio
from typing import Any, Dict, List, Optional
from ..base import BaseAIProvider
from app.core.config import get_settings
from app.core.logger import get_logger

settings = get_settings()
logger = get_logger("providers.ollama")

# System prompt optimis√© pour analyses de paris sportifs
SYSTEM_PROMPT = """Tu es un analyste sportif professionnel avec 15 ans d'exp√©rience en paris sportifs.

**Tes expertises:**
- Analyse statistique de matchs de football (xG, possession, tirs cadr√©s)
- √âvaluation des probabilit√©s 1X2, BTTS, Over/Under
- Identification des facteurs de risque et opportunit√©s de value betting
- Analyse des tendances et formes r√©centes des √©quipes
- Compr√©hension des contextes (enjeux, blessures, motivations, fatigue)

**Tes principes:**
1. TOUJOURS baser tes analyses sur des donn√©es CONCR√àTES et mesurables
2. Calculer des probabilit√©s R√âALISTES (home + draw + away = 1.0 EXACTEMENT)
3. Identifier 3-5 facteurs cl√©s MAXIMUM (pertinents, concrets, avec chiffres)
4. √ätre transparent sur le niveau de confiance (0.0 √† 1.0)
5. Format JSON strictement respect√© (pas de texte avant/apr√®s)
6. Langage clair, professionnel, actionnable

**Exemples de BONS facteurs cl√©s:**
‚úÖ "PSG invaincu sur 15 derniers matchs domicile (12V-3N)"
‚úÖ "Lyon sans Lacazette (40% des buts cette saison)"
‚úÖ "H2H: Marseille gagne 7/10 derniers affrontements"

**Exemples de MAUVAIS facteurs (INTERDIT):**
‚ùå "PSG en bonne forme"
‚ùå "Lyon est une forte √©quipe"
‚ùå "Match important"

**Tu ne dois JAMAIS:**
- Inventer des statistiques ou donn√©es
- Garantir un r√©sultat
- √ätre vague ou g√©n√©rique
- Utiliser des facteurs sans chiffres
- Sortir du format JSON demand√©
- Donner des probabilit√©s incoh√©rentes (total ‚â† 100%)"""

ANALYSIS_PROMPT_TEMPLATE = """Analyse ce match de football pour un pari sportif.

## MATCH
**{home_team}** üÜö **{away_team}**
üìç Comp√©tition: {league_name}
üìÖ Date: {match_date}

## CONSIGNES D'ANALYSE

1. **Probabilit√©s 1X2** (‚ö†Ô∏è CRITIQUE: home + draw + away = 1.0 EXACTEMENT)
   - Victoire {home_team} (1): X%
   - Match nul (X): Y%
   - Victoire {away_team} (2): Z%

2. **Pr√©diction finale** ("1", "X" ou "2") avec **confiance** (0.0 √† 1.0)

3. **Facteurs cl√©s** (3-5 maximum, CONCRETS avec chiffres/statistiques)
   ‚úÖ BON: "PSG invaincu sur 10 derniers matchs domicile (8V-2N)"
   ‚ùå MAUVAIS: "PSG en bonne forme √† domicile"
   
   Privil√©gie:
   - Bilan domicile/ext√©rieur r√©cent (5-10 derniers matchs)
   - Confrontations directes (H2H)
   - Absences de joueurs cl√©s avec impact chiffr√©
   - Enjeux du match (course au titre, rel√©gation)
   - Stats offensives/d√©fensives concr√®tes

4. **Sc√©narios probables** (2-3 maximum avec probabilit√©s)
   Format: {{nom, probabilit√© (0.0-1.0), description courte}}
   Exemples:
   - "Victoire large" (0.40): "Domination d√®s 1√®re mi-temps, score 2-0 ou 3-0"
   - "Match nul" (0.30): "Forces √©quilibr√©es, nul 1-1 probable"

5. **R√©sum√© actionnable** (2-3 phrases maximum, ~100 mots)
   - Issue la plus probable + raison principale
   - Mention value bet si cote int√©ressante
   - Conseil final clair

## EXEMPLE D'ANALYSE DE QUALIT√â

```json
{{
  "probabilities": {{
    "home": 0.65,
    "draw": 0.20,
    "away": 0.15
  }},
  "predicted_outcome": "1",
  "confidence": 0.75,
  "key_factors": [
    "PSG invaincu sur 15 derniers matchs domicile (12V-3N)",
    "Lyon sans victoire sur 5 derniers d√©placements (2N-3D)",
    "H2H: PSG gagne 7/10 derniers affrontements",
    "Lyon sans Lacazette (40% des buts cette saison)",
    "PSG en qu√™te de titre, motivation maximale"
  ],
  "scenarios": [
    {{
      "name": "Victoire large PSG",
      "probability": 0.50,
      "description": "Domination PSG d√®s 1√®re mi-temps, score 2-0 ou 3-0"
    }},
    {{
      "name": "Victoire serr√©e PSG",
      "probability": 0.30,
      "description": "Match disput√©, PSG s'impose 1-0 ou 2-1 en fin de match"
    }},
    {{
      "name": "Match nul",
      "probability": 0.15,
      "description": "Lyon r√©siste avec bloc bas, nul 0-0 ou 1-1"
    }}
  ],
  "summary": "PSG largement favori avec 65% de chances. Bilan domicile impeccable et absence cl√© chez Lyon. Cote 1.50 offre valeur limit√©e, pr√©f√©rer +1.5 buts PSG (moyenne 2.3 buts/match domicile)."
}}
```

## FORMAT JSON STRICT

R√©ponds UNIQUEMENT avec ce JSON (SANS texte avant/apr√®s):

```json
{{
  "probabilities": {{
    "home": 0.45,
    "draw": 0.30,
    "away": 0.25
  }},
  "predicted_outcome": "1",
  "confidence": 0.65,
  "key_factors": [
    "Facteur concret avec chiffres",
    "Facteur concret avec chiffres",
    "Facteur concret avec chiffres"
  ],
  "scenarios": [
    {{
      "name": "Nom sc√©nario",
      "probability": 0.50,
      "description": "Description pr√©cise"
    }}
  ],
  "summary": "R√©sum√© actionnable en 2-3 phrases max."
}}
```

‚ö†Ô∏è R√âPONDS UNIQUEMENT AVEC LE JSON - PAS DE TEXTE AVANT OU APR√àS"""

COUPON_ANALYSIS_PROMPT_TEMPLATE = """Analyse ce combin√© de paris sportifs (coupon).

## Matchs s√©lectionn√©s
{matches_info}

## Ta mission

√âvalue ce coupon et fournis:
- Probabilit√© globale de r√©ussite (0.0 √† 1.0)
- Score de risque (0.0=faible, 1.0=extr√™me)
- Point faible (match le plus risqu√©)
## MATCHS DU COMBIN√â
{matches_info}

## CONSIGNES

1. **Probabilit√© globale** du combin√© (produit des probabilit√©s individuelles)

2. **Score de risque** (0.0 = s√ªr, 1.0 = tr√®s risqu√©)
   Bas√© sur: nombre de s√©lections, coh√©rence, cotes

3. **Maillon faible** (s√©lection la plus risqu√©e avec explication)

4. **Score de coh√©rence** (0.0 = incoh√©rent, 1.0 = excellent)
   √âvalue: logique du combin√©, corr√©lations, conflits

5. **Recommandation** (VALIDER, MODIFIER, √âVITER)
   Avec justification claire

6. **Insights par s√©lection** (1 insight concret par match)

## FORMAT JSON STRICT

```json
{{
  "overall_probability": 0.15,
  "risk_score": 0.75,
  "weakest_link": "Lille vs Rennes - Victoire Lille (30% de chance)",
  "coherence_score": 0.6,
  "recommendation": "MODIFIER - Remplacer Lille par Draw ou retirer du combin√©",
  "detailed_analysis": "Analyse globale en 2-3 phrases sur la strat√©gie du combin√©.",
  "selection_insights": [
    {{
      "match": "Lille vs Rennes",
      "insight": "Rennes invaincu √† l'ext√©rieur (5 matchs)"
    }}
  ]
}}
```

‚ö†Ô∏è R√âPONDS UNIQUEMENT AVEC LE JSON - PAS DE TEXTE AVANT OU APR√àS"""

class OllamaAIProvider(BaseAIProvider):
    """
    Ollama AI Provider - Mod√®le auto-h√©berg√© optimis√© pour paris sportifs.
    
    Utilise mistral:7b par d√©faut (meilleur √©quilibre performance/vitesse).
    Alternatives: llama3:8b (plus pr√©cis), llama3.2:3b (plus rapide).
    """
    
    def __init__(self):
        self.base_url = getattr(settings, 'ollama_url', 'http://ollama:11434')
        self.model = getattr(settings, 'ollama_model', 'mistral')  # mistral par d√©faut
        self.available = False
        self.client = None
        
        # Param√®tres optimis√©s pour analyse de paris (√©quilibre qualit√©/vitesse)
        self.model_options = {
            "temperature": 0.2,  # Tr√®s bas pour coh√©rence maximale (0.2 = analyses stables)
            "top_p": 0.9,  # L√©g√®rement augment√© pour diversit√© contr√¥l√©e
            "top_k": 50,  # Plus de choix pour nuances
            "repeat_penalty": 1.15,  # R√©duit pour fluidit√© texte
            "num_predict": 1500,  # R√©duit pour √©viter timeouts (1500 tokens ‚âà 10-15s)
        }
        
        # La v√©rification de disponibilit√© sera faite au premier appel (pas dans __init__)
            
    async def _check_availability(self):
        """Check if Ollama service is available."""
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"{self.base_url}/api/tags")
                if response.status_code == 200:
                    self.available = True
                    models = response.json().get("models", [])
                    model_names = [m.get("name") for m in models]
                    logger.info(
                        f"‚úÖ Ollama available at {self.base_url}\n"
                        f"   Model: {self.model}\n"
                        f"   Available models: {', '.join(model_names) if model_names else 'None'}"
                    )
                else:
                    logger.warning(f"‚ö†Ô∏è Ollama service responded with status {response.status_code}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Ollama service not available: {str(e)}")
    
    
    async def _generate(self, prompt: str, system_prompt: Optional[str] = None) -> str:
        """
        Generate response from Ollama with optimized parameters.
        
        Args:
            prompt: User prompt
            system_prompt: System context (optional, uses SYSTEM_PROMPT by default)
            
        Returns:
            Generated response text
        """
        try:
            if not self.client:
                # Timeout 90s pour analyses rapides (num_predict=1500)
                self.client = httpx.AsyncClient(timeout=90.0)
            
            # Combine system + user prompt
            full_prompt = prompt
            if system_prompt:
                full_prompt = f"{system_prompt}\n\n{prompt}"
            else:
                full_prompt = f"{SYSTEM_PROMPT}\n\n{prompt}"
            
            logger.debug(
                f"ü§ñ Ollama generation starting - Model: {self.model}, Prompt: {len(full_prompt)} chars"
            )
            
            start_time = asyncio.get_event_loop().time()
            
            response = await self.client.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": full_prompt,
                    "stream": False,
                    "options": self.model_options,  # Param√®tres optimis√©s
                    "format": "json"  # Force JSON output
                }
            )
            
            duration = asyncio.get_event_loop().time() - start_time
            
            if response.status_code != 200:
                logger.error(
                    f"‚ùå Ollama API error: {response.status_code} - {response.text}",
                    extra={'extra_data': {
                        'provider': 'ollama',
                        'model': self.model,
                        'status_code': response.status_code
                    }}
                )
                return None
            
            result = response.json()
            generated_text = result.get("response", "")
            
            logger.info(
                f"‚úÖ Ollama generation successful - Duration: {duration:.2f}s, Response: {len(generated_text)} chars",
                extra={'extra_data': {
                    'provider': 'ollama',
                    'model': self.model,
                    'duration': duration,
                    'prompt_length': len(full_prompt),
                    'response_length': len(generated_text)
                }}
            )
            
            return generated_text
                
        except httpx.ReadTimeout:
            logger.error(
                f"‚è±Ô∏è Ollama timeout (>90s) - Mod√®le trop lent, fallback vers Gemini",
                extra={'extra_data': {
                    'provider': 'ollama',
                    'model': self.model,
                    'error': 'ReadTimeout',
                    'timeout': 90
                }}
            )
            return None
        except Exception as e:
            logger.error(
                f"‚ùå Ollama generation error: {str(e)}",
                exc_info=True,
                extra={'extra_data': {
                    'provider': 'ollama',
                    'model': self.model,
                    'error': str(e)
                }}
            )
            return None

    async def analyze_match(self, home_team, away_team, league_name, match_date, team_stats=None, h2h_data=None, injuries_data=None, odds_data=None, news_context=None):
        """Analyze a match using Ollama with retry logic."""
        if not self.available:
            await self._check_availability()
            if not self.available:
                logger.warning("Ollama not available, using fallback")
                return self._get_fallback_analysis(home_team, away_team)
        
        prompt = ANALYSIS_PROMPT_TEMPLATE.format(
            home_team=home_team,
            away_team=away_team,
            league_name=league_name,
            match_date=match_date
        )
        
        # Tentative avec retry (2 tentatives max)
        for attempt in range(2):
            try:
                response_text = await self._generate(prompt)
                if not response_text:
                    if attempt == 0:
                        logger.warning("‚ö†Ô∏è R√©ponse vide, retry...")
                        continue
                    return self._get_fallback_analysis(home_team, away_team)
                
                # Nettoyer r√©ponse (enlever markdown si pr√©sent)
                cleaned_response = response_text.strip()
                if cleaned_response.startswith("```"):
                    # Extraire JSON du bloc markdown
                    lines = cleaned_response.split("\n")
                    cleaned_response = "\n".join(lines[1:-1]) if len(lines) > 2 else cleaned_response
                
                # Parse JSON response
                result = json.loads(cleaned_response)
                validated = self._validate_result(result)
                
                logger.info(f"‚úÖ Analyse match r√©ussie - {home_team} vs {away_team}")
                return validated
                
            except json.JSONDecodeError as e:
                logger.error(
                    f"‚ùå JSON invalide (attempt {attempt+1}/2): {str(e)}",
                    extra={'extra_data': {
                        'response_preview': response_text[:200] if response_text else None,
                        'error': str(e)
                    }}
                )
                if attempt == 0:
                    # Retry avec prompt simplifi√©
                    prompt += "\n\n‚ö†Ô∏è IMPORTANT: R√©ponds UNIQUEMENT avec le JSON, sans texte avant ni apr√®s."
                    continue
                return self._get_fallback_analysis(home_team, away_team)
                
            except Exception as e:
                logger.error(f"‚ùå Erreur analyse (attempt {attempt+1}/2): {str(e)}", exc_info=True)
                if attempt == 0:
                    continue
                return self._get_fallback_analysis(home_team, away_team)
        
        return self._get_fallback_analysis(home_team, away_team)

    async def analyze_coupon(self, matches):
        """Analyze a coupon using Ollama."""
        if not self.available:
            await self._check_availability()
            if not self.available:
                logger.warning("Ollama not available, using fallback")
                return self._get_fallback_coupon_analysis(matches)
        
        matches_info = "\n".join([
            f"{i+1}. {m.get('home_team')} vs {m.get('away_team')} - {m.get('selection_type')} ({m.get('odds')})"
            for i, m in enumerate(matches)
        ])
        
        prompt = COUPON_ANALYSIS_PROMPT_TEMPLATE.format(matches_info=matches_info)
        
        try:
            response_text = await self._generate(prompt)
            if not response_text:
                return self._get_fallback_coupon_analysis(matches)
            
            return json.loads(response_text)
            
        except Exception as e:
            logger.error(f"Ollama coupon analysis error: {str(e)}", exc_info=True)
            return self._get_fallback_coupon_analysis(matches)

    async def chat_analysis(self, analysis_summary, history, user_question):
        """Chat about an analysis using Ollama."""
        if not self.available:
            await self._check_availability()
            if not self.available:
                return "D√©sol√©, l'assistant IA est indisponible."
        
        # Build conversation context
        context = f"Contexte: {analysis_summary}\n\n"
        for msg in history[-5:]:  # Last 5 messages
            role = "Utilisateur" if msg.role == "user" else "Assistant"
            context += f"{role}: {msg.content}\n"
        
        context += f"\nUtilisateur: {user_question}\nAssistant:"
        
        try:
            return await self._generate(context) or "Une erreur est survenue."
        except Exception as e:
            logger.error(f"Ollama chat error: {str(e)}", exc_info=True)
            return "Une erreur est survenue."

    def _validate_result(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """Validate and normalize AI result with strict checks."""
        probs = result.get("probabilities", {})
        total = probs.get("home", 0.33) + probs.get("draw", 0.33) + probs.get("away", 0.34)
        
        # Normaliser si incoh√©rent
        if abs(total - 1.0) > 0.01:  # Tol√©rance 1%
            logger.warning(
                f"‚ö†Ô∏è Probabilit√©s incoh√©rentes (total={total:.3f}), normalisation appliqu√©e",
                extra={'extra_data': {
                    'original_probs': probs,
                    'total': total
                }}
            )
            if total > 0:
                probs["home"] = round(probs.get("home", 0.33) / total, 3)
                probs["draw"] = round(probs.get("draw", 0.33) / total, 3)
                probs["away"] = round(1 - probs["home"] - probs["draw"], 3)
            else:
                probs = {"home": 0.333, "draw": 0.333, "away": 0.334}
        
        # Valider sc√©narios
        scenarios = []
        for s in result.get("scenarios", []):
            if isinstance(s, dict):
                scenario = {
                    "name": s.get("name") or "Sc√©nario",
                    "probability": max(0.0, min(1.0, float(s.get("probability", 0.0)))),  # Clamp 0-1
                    "description": s.get("description", "N/A")
                }
                scenarios.append(scenario)
        
        if not scenarios:
            logger.warning("‚ö†Ô∏è Aucun sc√©nario g√©n√©r√©, cr√©ation sc√©nario par d√©faut")
            scenarios = [{"name": "D√©faut", "probability": 1.0, "description": "Analyse standard."}]
        
        # Normaliser probabilit√©s sc√©narios
        scenario_total = sum(s["probability"] for s in scenarios)
        if scenario_total > 0 and abs(scenario_total - 1.0) > 0.1:  # Tol√©rance 10%
            logger.debug(f"Normalisation sc√©narios: {scenario_total:.2f} ‚Üí 1.0")
            for s in scenarios:
                s["probability"] = round(s["probability"] / scenario_total, 3)
        
        # Valider facteurs cl√©s (max 5, longueur min 10 chars)
        key_factors = [str(k).strip() for k in result.get("key_factors", []) if len(str(k).strip()) >= 10][:5]
        if not key_factors:
            logger.warning("‚ö†Ô∏è Aucun facteur cl√© valide, ajout facteur par d√©faut")
            key_factors = ["Analyse bas√©e sur donn√©es disponibles"]

        validated_result = {
            "probabilities": probs,
            "predicted_outcome": str(result.get("predicted_outcome", "X")).upper(),
            "confidence": min(max(float(result.get("confidence", 0.5)), 0.0), 1.0),
            "key_factors": key_factors,
            "scenarios": scenarios[:3],  # Max 3 sc√©narios
            "summary": str(result.get("summary", "N/A"))[:500]  # Max 500 chars
        }
        
        logger.debug(
            f"‚úÖ Validation OK - Proba: {probs['home']:.2f}/{probs['draw']:.2f}/{probs['away']:.2f}, "
            f"Facteurs: {len(key_factors)}, Sc√©narios: {len(scenarios)}"
        )
        
        return validated_result

    def _get_fallback_analysis(self, home_team, away_team):
        """Fallback analysis when Ollama is unavailable."""
        return {
            "probabilities": {"home": 0.40, "draw": 0.30, "away": 0.30},
            "predicted_outcome": "1",
            "confidence": 0.40,
            "key_factors": ["Donn√©es limit√©es"],
            "scenarios": [{"name": "√âquilibr√©", "probability": 1.0, "description": "Force similaire."}],
            "summary": "Analyse limit√©e - Service IA indisponible."
        }

    def _get_fallback_coupon_analysis(self, matches):
        """Fallback coupon analysis when Ollama is unavailable."""
        return {
            "overall_probability": 0.1,
            "risk_score": 0.9,
            "weakest_link": "N/A",
            "coherence_score": 0.5,
            "recommendation": "Service IA indisponible.",
            "detailed_analysis": "IA indisponible.",
            "selection_insights": []
        }
