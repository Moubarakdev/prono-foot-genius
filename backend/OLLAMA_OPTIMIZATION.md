# Optimisation Ollama pour CouponFoot

## ðŸŽ¯ Vue d'ensemble

CouponFoot utilise Ollama comme solution d'IA auto-hÃ©bergÃ©e pour l'analyse de paris sportifs. Ce document explique les optimisations mises en place et comment les configurer.

## ðŸš€ Installation rapide

```bash
# Windows (PowerShell)
cd backend
.\setup_ollama.ps1

# Linux/Mac
cd backend
bash setup_ollama.sh
```

## ðŸ“Š ModÃ¨les disponibles

### âœ… Mistral 7B (RecommandÃ© - Par dÃ©faut)
- **Taille**: ~4.1 GB
- **RAM nÃ©cessaire**: 8 GB
- **Performance**: Excellent Ã©quilibre vitesse/qualitÃ©
- **Temps de rÃ©ponse**: 5-15 secondes
- **Utilisation**: Analyse standard de matchs et coupons

**Installation:**
```bash
docker exec api-football-ollama-1 ollama pull mistral
```

### â­ Llama3 8B (Plus prÃ©cis)
- **Taille**: ~4.7 GB
- **RAM nÃ©cessaire**: 10 GB
- **Performance**: Meilleure qualitÃ© d'analyse
- **Temps de rÃ©ponse**: 10-20 secondes
- **Utilisation**: Analyses complexes, gros coupons

**Installation:**
```bash
docker exec api-football-ollama-1 ollama pull llama3:8b
```

### âš¡ Llama3.2 3B (Plus rapide)
- **Taille**: ~2.0 GB
- **RAM nÃ©cessaire**: 4 GB
- **Performance**: Rapide mais moins prÃ©cis
- **Temps de rÃ©ponse**: 3-8 secondes
- **Utilisation**: Tests, dÃ©veloppement

**Installation:**
```bash
docker exec api-football-ollama-1 ollama pull llama3.2
```

## âš™ï¸ ParamÃ¨tres optimisÃ©s

Les paramÃ¨tres suivants sont configurÃ©s dans `ollama.py` pour maximiser la cohÃ©rence des analyses:

```python
model_options = {
    "temperature": 0.3,      # Bas pour cohÃ©rence et reproductibilitÃ©
    "top_p": 0.85,           # RÃ©duit pour Ã©viter rÃ©ponses alÃ©atoires
    "top_k": 40,             # Limite les choix de tokens
    "repeat_penalty": 1.2,   # Ã‰vite les rÃ©pÃ©titions
    "num_predict": 3000,     # Suffisant pour analyses dÃ©taillÃ©es
}
```

### ðŸ“– Explication des paramÃ¨tres

#### Temperature (0.0 - 2.0)
- **0.0-0.3**: TrÃ¨s cohÃ©rent, prÃ©visible âœ… (utilisÃ© pour paris)
- **0.4-0.7**: Ã‰quilibrÃ©
- **0.8-2.0**: CrÃ©atif, alÃ©atoire

#### Top P (0.0 - 1.0)
- **0.7-0.85**: Focus sur tokens probables âœ…
- **0.9-1.0**: Plus de diversitÃ©

#### Top K (1 - 100)
- **20-40**: Limite les choix âœ… (cohÃ©rence)
- **60-100**: Plus de variÃ©tÃ©

#### Num Predict (tokens)
- **2000-3000**: Analyses dÃ©taillÃ©es âœ…
- **4000+**: TrÃ¨s longues analyses (lent)

## ðŸŽ¨ Prompts optimisÃ©s

### System Prompt
Le `SYSTEM_PROMPT` dÃ©finit le rÃ´le et les rÃ¨gles:
```python
Tu es un analyste sportif professionnel avec 15 ans d'expÃ©rience en paris sportifs.

**Tes expertises:**
- Analyse statistique de matchs de football
- Ã‰valuation des probabilitÃ©s 1X2, BTTS, Over/Under
- Identification des facteurs de risque et opportunitÃ©s
...
```

**Avantages:**
- âœ… DÃ©finit clairement le contexte
- âœ… Impose des rÃ¨gles strictes (JSON, somme = 100%, etc.)
- âœ… Ã‰vite les rÃ©ponses hors sujet

### Prompt d'analyse de match
Structure claire avec consignes prÃ©cises:
- Format Markdown avec emojis pour lisibilitÃ©
- Contraintes explicites (TOTAL = 100%, 3-5 facteurs max)
- Exemples de format JSON attendu
- Avertissement final: **RÃ‰PONDS UNIQUEMENT AVEC LE JSON**

**RÃ©sultat:**
- âœ… 95%+ de rÃ©ponses au format JSON valide
- âœ… Analyses cohÃ©rentes et structurÃ©es
- âœ… ProbabilitÃ©s rÃ©alistes

## ðŸ”§ Configuration avancÃ©e

### Changer de modÃ¨le

**Option 1: Via .env (recommandÃ©)**
```bash
# Ã‰diter backend/.env
AI_PROVIDER=ollama
OLLAMA_MODEL=mistral  # ou llama3:8b, llama3.2, etc.

# RedÃ©marrer l'API
docker-compose restart api
```

**Option 2: Directement dans le code**
```python
# backend/app/providers/ai/ollama.py
def __init__(self):
    self.model = "llama3:8b"  # Changer ici
    ...
```

### Ajuster les paramÃ¨tres

Si vous trouvez les analyses trop rigides ou trop alÃ©atoires:

```python
# backend/app/providers/ai/ollama.py

# Pour plus de crÃ©ativitÃ©
self.model_options = {
    "temperature": 0.5,   # Augmenter (0.3 â†’ 0.5)
    "top_p": 0.9,         # Augmenter (0.85 â†’ 0.9)
    "top_k": 60,          # Augmenter (40 â†’ 60)
    ...
}

# Pour plus de cohÃ©rence
self.model_options = {
    "temperature": 0.2,   # Diminuer (0.3 â†’ 0.2)
    "top_p": 0.75,        # Diminuer (0.85 â†’ 0.75)
    "top_k": 30,          # Diminuer (40 â†’ 30)
    ...
}
```

### Timeout

Si les analyses prennent trop de temps:
```python
# backend/app/providers/ai/ollama.py
self.client = httpx.AsyncClient(timeout=120.0)  # Diminuer si besoin
```

## ðŸ“ˆ Monitoring

### VÃ©rifier que Ollama fonctionne

```bash
# Check status
docker ps | grep ollama

# Check logs
docker logs api-football-ollama-1

# Test API
curl http://localhost:11434/api/tags

# Voir les modÃ¨les installÃ©s
docker exec api-football-ollama-1 ollama list
```

### Logs dans l'application

Les logs de l'API montrent:
```
âœ… Ollama available at http://ollama:11434
   Model: mistral
   Available models: mistral, llama3:8b

âœ… Ollama generation successful
   Duration: 8.32s
   Response length: 1245 chars
```

### Erreurs courantes

**"Ollama service not available"**
```bash
# RedÃ©marrer Ollama
docker-compose restart ollama

# VÃ©rifier qu'il dÃ©marre bien
docker logs -f api-football-ollama-1
```

**"Model 'mistral' not found"**
```bash
# TÃ©lÃ©charger le modÃ¨le
docker exec api-football-ollama-1 ollama pull mistral
```

**"Generation timeout"**
- Augmenter le timeout dans `ollama.py`
- Ou utiliser un modÃ¨le plus petit (llama3.2)

## ðŸŽ¯ Benchmarks

Sur une machine moyenne (16GB RAM, CPU i7):

| ModÃ¨le | Temps moyen | QualitÃ© | RAM utilisÃ©e |
|--------|-------------|---------|--------------|
| Llama3.2 3B | 5s | â­â­â­ | ~4 GB |
| **Mistral 7B** | **10s** | **â­â­â­â­** | **~8 GB** |
| Llama3 8B | 15s | â­â­â­â­â­ | ~10 GB |

**Recommandation:** Mistral 7B (meilleur rapport qualitÃ©/vitesse)

## ðŸ”„ Migration depuis Gemini

Si vous utilisez encore Gemini et voulez passer Ã  Ollama:

1. **Installer Ollama:**
```bash
cd backend
.\setup_ollama.ps1  # Windows
# ou
bash setup_ollama.sh  # Linux/Mac
```

2. **Changer le provider dans .env:**
```bash
AI_PROVIDER=ollama  # au lieu de gemini
```

3. **RedÃ©marrer:**
```bash
docker-compose restart api
```

**Avantages d'Ollama vs Gemini:**
- âœ… IllimitÃ© (pas de quota)
- âœ… Gratuit (self-hosted)
- âœ… PrivÃ© (donnÃ©es en local)
- âœ… Personnalisable (prompts, paramÃ¨tres)
- âŒ Plus lent (mais suffisant)
- âŒ NÃ©cessite RAM serveur

## ðŸŽ“ Bonnes pratiques

### âœ… DO
- Utiliser Mistral 7B en production
- Monitorer les temps de rÃ©ponse
- Ajuster temperature si analyses trop rigides
- Cacher les rÃ©sultats en Redis (TTL: 5-10 min)
- Mettre Ã  jour les modÃ¨les rÃ©guliÃ¨rement

### âŒ DON'T
- Ne pas utiliser llama3.2 en production (trop imprÃ©cis)
- Ne pas augmenter temperature > 0.5 (analyses incohÃ©rentes)
- Ne pas set num_predict > 4000 (trÃ¨s lent)
- Ne pas oublier de pull le modÃ¨le avant utilisation
- Ne pas ignorer les logs d'erreur

## ðŸ“š Ressources

- [Ollama Documentation](https://github.com/ollama/ollama)
- [Mistral AI](https://mistral.ai/)
- [Llama 3](https://ai.meta.com/llama/)
- [Model Parameters Guide](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#parameter)

## ðŸ†˜ Support

Si vous rencontrez des problÃ¨mes:
1. VÃ©rifier les logs: `docker logs api-football-ollama-1`
2. VÃ©rifier les logs de l'API: `docker logs api-football-api-1`
3. Tester l'API Ollama: `curl http://localhost:11434/api/tags`
4. RedÃ©marrer: `docker-compose restart ollama api`

---

**DerniÃ¨re mise Ã  jour:** 2024
**Version:** 1.0
